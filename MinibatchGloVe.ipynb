{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trials to minibatch GloVe\n",
    "#### Eduardo Fierro \n",
    "####Â Based on NLP - Homework 1 (Much of the code was copied)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all packages\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import nltk.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File params\n",
    "main_data_dir = '/Users/eduardofierro/Google Drive/TercerSemetre/Optimization/Project/Data'\n",
    "minibatch = 400 #(of 4150 train files example)\n",
    "context_window = 4 # Context window to train embeddings. \n",
    "top_k = 250 # Vocabulary size\n",
    "\n",
    "embedding_dim = 10\n",
    "batch_size = 1024\n",
    "learning_rate = 1.0\n",
    "num_epochs = 2000\n",
    "alpha = 0.75 # GloVe model param\n",
    "xmax = 50 # GloVe model param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I : Data I/O\n",
    "Read data from disk and parse them into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change directory for data\n",
    "np.random.seed(1234)\n",
    "train_examples = np.loadtxt(main_data_dir + \"/test.txt\", dtype=\"str\")\n",
    "if minibatch:\n",
    "    train_examples = np.random.choice(train_examples, minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readFile(filename, path = main_data_dir): \n",
    "    \n",
    "    with open(path + \"/TXTsOriginal/\" + filename) as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "def replaceLineBreaks(text): \n",
    "    \n",
    "    text = re.sub(\"\\n   \", \" \", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    return(text)\n",
    "\n",
    "def sentenceBreak(text):\n",
    "    \n",
    "    text = nltk.sent_tokenize(text)\n",
    "    return(text)\n",
    "\n",
    "def loadCorpus(listFiles): \n",
    "    \n",
    "    print(\"Building Corpus...\")\n",
    "    all_sentences = []\n",
    "    \n",
    "    for i, file in enumerate(listFiles): \n",
    "        text = readFile(file)\n",
    "        text = replaceLineBreaks(text)\n",
    "        text = sentenceBreak(text)\n",
    "        all_sentences.extend(text)\n",
    "        \n",
    "        if i%100 == 0: \n",
    "            print(\"{}/{} advance\".format(i, len(listFiles)))\n",
    "    \n",
    "    print(\"Corpus Ready!!\")    \n",
    "    return(all_sentences)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Corpus...\n",
      "0/400 advance\n",
      "100/400 advance\n",
      "200/400 advance\n",
      "300/400 advance\n",
      "Corpus Ready!!\n"
     ]
    }
   ],
   "source": [
    "corpus_sentences = loadCorpus(train_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II : Coocurrance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    string = string.lower()\n",
    "    return string.split()\n",
    "\n",
    "word_counter = collections.Counter()\n",
    "for example in corpus_sentences:\n",
    "    word_counter.update(tokenize(example))\n",
    "vocabulary = [pair[0] for pair in word_counter.most_common()[0:top_k]]\n",
    "index_to_word_map = dict(enumerate(vocabulary))\n",
    "word_to_index_map = dict([(index_to_word_map[index], index) for index in index_to_word_map])\n",
    "\n",
    "def extract_cooccurrences(dataset, word_map, amount_of_context=context_window):\n",
    "    num_words = len(vocabulary)\n",
    "    cooccurrences = np.zeros((num_words, num_words))\n",
    "    nonzero_pairs = set()\n",
    "    for example in dataset:\n",
    "        words = tokenize(example)\n",
    "        for target_index in range(len(words)):\n",
    "            target_word = words[target_index]\n",
    "            if target_word not in word_to_index_map:\n",
    "                continue\n",
    "            target_word_index = word_to_index_map[target_word]\n",
    "            min_context_index = max(0, target_index - amount_of_context)\n",
    "            max_word = min(len(words), target_index + amount_of_context + 1)\n",
    "            for context_index in list(range(min_context_index, target_index)) + \\\n",
    "            list(range(target_index + 1, max_word)):\n",
    "                context_word = words[context_index]\n",
    "                if context_word not in word_to_index_map:\n",
    "                    continue\n",
    "                context_word_index = word_to_index_map[context_word]\n",
    "                cooccurrences[target_word_index][context_word_index] += 1.0\n",
    "                nonzero_pairs.add((target_word_index, context_word_index))\n",
    "    return cooccurrences, list(nonzero_pairs)\n",
    "                \n",
    "cooccurrences, nonzero_pairs = extract_cooccurrences(corpus_sentences, vocabulary)\n",
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III: Batchify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(nonzero_pairs, cooccurrences, batch_size):\n",
    "    start = -1 * batch_size\n",
    "    dataset_size = len(nonzero_pairs)\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        word_i = []\n",
    "        word_j = []\n",
    "        counts = []\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [nonzero_pairs[index] for index in batch_indices]\n",
    "        for k in batch:\n",
    "            counts.append(cooccurrences[k])\n",
    "            word_i.append(k[0])\n",
    "            word_j.append(k[1])\n",
    "        yield [counts, word_i, word_j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IV: Evaluation Metric\n",
    "\n",
    "NONE - Will manually check later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part V: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Glove(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, vocab_size, batch_size):\n",
    "        \n",
    "        super(Glove, self).__init__()    \n",
    "        \n",
    "        self.word_embeddings = None\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.wi = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.wj = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        \n",
    "        self.bi = nn.Embedding(self.vocab_size, 1)\n",
    "        self.bj = nn.Embedding(self.vocab_size, 1)\n",
    "        \n",
    "        self.init_weights()           \n",
    "    \n",
    "    def forward(self, new_wi, new_wj):      \n",
    "        \n",
    "        out_wi = torch.squeeze(self.wi(new_wi))\n",
    "        out_wj = torch.squeeze(self.wj(new_wj))\n",
    "        out_bi = torch.squeeze(self.bi(new_wi))\n",
    "        out_bj = torch.squeeze(self.bj(new_wj))\n",
    "        \n",
    "        return out_wi, out_wj, out_bi, out_bj\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        \n",
    "        initrange = 0.1\n",
    "        init_vars = [self.wi, self.wj, self.bi, self.bj]\n",
    "        \n",
    "        for var in init_vars:\n",
    "            var.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "\n",
    "    def add_embeddings(self):\n",
    "        \n",
    "        self.word_embeddings = self.wi.weight + self.wj.weight\n",
    "    \n",
    "    def get_embeddings(self, index):\n",
    "        if self.word_embeddings is None:\n",
    "            add_embeddings()\n",
    "\n",
    "        return self.word_embeddings.data[index, :].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part VI: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_loop(training_set, batch_size, num_epochs, model, optim, data_iter, xmax, alpha):\n",
    "    step = 0\n",
    "    epoch = 0\n",
    "    losses = []\n",
    "    total_batches = int(len(training_set) / batch_size)\n",
    "        \n",
    "    while epoch <= num_epochs:\n",
    "        model.train()\n",
    "        counts, words, co_words = next(data_iter)        \n",
    "        words_var = Variable(torch.LongTensor(words))\n",
    "        co_words_var = Variable(torch.LongTensor(co_words))\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        wi, wj, bi, bj = model(words_var, co_words_var)\n",
    "        counts_var = Variable(torch.FloatTensor([counts]))\n",
    "        counts_fx = [1 if x >= xmax else (x/xmax)**alpha for x in counts]\n",
    "        counts_fx_var = Variable(torch.FloatTensor([counts_fx]))\n",
    "        loss = sum(torch.t(torch.mul((torch.mm(wi, torch.t(wj)).diag() + bi + bj - torch.log(counts_var))**2, counts_fx_var)))\n",
    "                \n",
    "        losses.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % total_batches == 0:\n",
    "            epoch += 1\n",
    "            if epoch % 25 == 0:\n",
    "                word_emebddings = model.add_embeddings()\n",
    "                print( \"Epoch:\", (epoch),\"/\", (num_epochs) , \"Avg Loss:\", np.mean(losses)/(total_batches*epoch))\n",
    "        \n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part VI: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove = Glove(embedding_dim, vocab_size, batch_size)\n",
    "glove.init_weights()\n",
    "optimizer = torch.optim.Adadelta(glove.parameters(), lr=learning_rate)\n",
    "data_iter = batch_iter(nonzero_pairs, cooccurrences, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Avg Loss: 0.81349013279\n",
      "Epoch: 50 Avg Loss: 0.262637839071\n",
      "Epoch: 75 Avg Loss: 0.13821094676\n",
      "Epoch: 100 Avg Loss: 0.0882147314119\n",
      "Epoch: 125 Avg Loss: 0.0626649726497\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d17158fa67ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-882021215625>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(training_set, batch_size, num_epochs, model, optim, data_iter, xmax, alpha)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_loop(corpus_sentences, batch_size, num_epochs, glove, optimizer, data_iter, xmax, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
