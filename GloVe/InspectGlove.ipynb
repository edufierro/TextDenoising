{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect GloVe\n",
    "\n",
    "### @edufierro\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/eduardofierro/Google Drive/TercerSemetre/Optimization/Project/Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove = pickle.load( open(data_dir +  \"GloVe_at_1900.p\", \"rb\" ) )\n",
    "index_to_word_map = pickle.load( open(data_dir +  \"index_to_word_map.p\", \"rb\" ) )\n",
    "word_to_index_map = pickle.load( open(data_dir +  \"word_to_index_map.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_glove = np.copy(glove)\n",
    "for col in range(0, normalized_glove.shape[0]):\n",
    "    normalized_glove[col, :] = normalized_glove[col, :] / np.linalg.norm(normalized_glove[col, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def similarity(matrix, word_one, word_two):\n",
    "    \n",
    "    vec_one = matrix[word_to_index_map[word_one]].reshape(1, -1)\n",
    "    vec_two = matrix[word_to_index_map[word_two]].reshape(1, -1)\n",
    "    return float(cosine_similarity(vec_one, vec_two))\n",
    "\n",
    "def nearest(matrix, word): \n",
    "    \n",
    "    nearest_val = similarity(matrix, word, index_to_word_map[0])\n",
    "    nearest_index = 0\n",
    "    \n",
    "    for x in range(1, matrix.shape[0]):\n",
    "        score = similarity(matrix, word, index_to_word_map[x])\n",
    "        if score < nearest_val: \n",
    "            nearest_val = score\n",
    "            nearest_index = x\n",
    "    \n",
    "    return nearest_index\n",
    "\n",
    "def nearest_from_vector(matrix, vec, k = 5): \n",
    "    \n",
    "    vec = vec.reshape(1, -1)\n",
    "    all_scores = []\n",
    "    \n",
    "    for x in range(0, matrix.shape[0]):\n",
    "        all_scores.append((x,float(cosine_similarity(vec, matrix[x].reshape(1, -1)))))\n",
    "\n",
    "    all_scores = sorted(all_scores, key=lambda key: key[1])    \n",
    "    all_scores = all_scores[0:k]\n",
    "    return [all_scores[x][0] for x in range(0, k)]\n",
    "\n",
    "def word_analogy(matrix, word_a, word_b, word_c, k=5):\n",
    "    \"\"\"\n",
    "    Function that solves problem word_a to word_b = word_c to ?\n",
    "    Remember to use normalized_embeddings, why?\n",
    "    @param word_a, word_b, word_c: string\n",
    "    @param k: top k candidates to return\n",
    "    \"\"\"\n",
    "    \n",
    "    vec = matrix[word_to_index_map[word_b]] - matrix[word_to_index_map[word_a]] + matrix[word_to_index_map[word_c]]\n",
    "    nearest = nearest_from_vector(matrix, vec, k = k)\n",
    "    words = []\n",
    "    for x in range(0, k):\n",
    "        words.append(index_to_word_map[nearest[x]])\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
