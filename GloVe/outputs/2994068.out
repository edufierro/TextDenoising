Namespace(alpha=0.75, batchSize=1024, context_window=8, disable_cuda=False, embedding_dim=300, learning_rate=0.01, main_data_dir='/scratch/eff254/Optimization/Data/', minibatch=10000, num_epochs=2000, numpy_random_seed=1234, top_k=10000, xmax=50)
True
Building Corpus...
0/4150 advance
100/4150 advance
200/4150 advance
300/4150 advance
400/4150 advance
500/4150 advance
600/4150 advance
700/4150 advance
800/4150 advance
900/4150 advance
1000/4150 advance
1100/4150 advance
1200/4150 advance
1300/4150 advance
1400/4150 advance
1500/4150 advance
1600/4150 advance
1700/4150 advance
1800/4150 advance
1900/4150 advance
2000/4150 advance
2100/4150 advance
2200/4150 advance
2300/4150 advance
2400/4150 advance
2500/4150 advance
2600/4150 advance
2700/4150 advance
2800/4150 advance
2900/4150 advance
3000/4150 advance
3100/4150 advance
3200/4150 advance
3300/4150 advance
3400/4150 advance
3500/4150 advance
3600/4150 advance
3700/4150 advance
3800/4150 advance
3900/4150 advance
4000/4150 advance
4100/4150 advance
Corpus Ready!!
Building cooccurrences matrix ...
Training Model...
Epoch: 25 / 2000 Avg Loss: 0.129585284384
Epoch: 50 / 2000 Avg Loss: 0.0632455953464
Epoch: 75 / 2000 Avg Loss: 0.041175225965
Epoch: 100 / 2000 Avg Loss: 0.030151504364
Epoch: 125 / 2000 Avg Loss: 0.0235407957538
Epoch: 150 / 2000 Avg Loss: 0.0191480224986
Epoch: 175 / 2000 Avg Loss: 0.0160141843476
Epoch: 200 / 2000 Avg Loss: 0.0136621002576
Epoch: 225 / 2000 Avg Loss: 0.0118306901286
Epoch: 250 / 2000 Avg Loss: 0.0103601555857
Epoch: 275 / 2000 Avg Loss: 0.009147261928
Epoch: 300 / 2000 Avg Loss: 0.00812479776844
Epoch: 325 / 2000 Avg Loss: 0.00724703239336
Epoch: 350 / 2000 Avg Loss: 0.00648336030704
Epoch: 375 / 2000 Avg Loss: 0.00581575773032
Epoch: 400 / 2000 Avg Loss: 0.00523330300239
Epoch: 425 / 2000 Avg Loss: 0.00472942517086
Epoch: 450 / 2000 Avg Loss: 0.00429566111159
Epoch: 475 / 2000 Avg Loss: 0.00392135285599
Epoch: 500 / 2000 Avg Loss: 0.00359717120696
Epoch: 525 / 2000 Avg Loss: 0.00331414203123
Epoch: 550 / 2000 Avg Loss: 0.00306582007716
Epoch: 575 / 2000 Avg Loss: 0.00284650602889
Epoch: 600 / 2000 Avg Loss: 0.00265180238736
Epoch: 625 / 2000 Avg Loss: 0.00247804984783
Epoch: 650 / 2000 Avg Loss: 0.00232226865459
Epoch: 675 / 2000 Avg Loss: 0.00218182589961
Epoch: 700 / 2000 Avg Loss: 0.00205491662907
Epoch: 725 / 2000 Avg Loss: 0.00193969227981
Epoch: 750 / 2000 Avg Loss: 0.00183474161498
Epoch: 775 / 2000 Avg Loss: 0.00173883263819
Epoch: 800 / 2000 Avg Loss: 0.00165091471538
Epoch: 825 / 2000 Avg Loss: 0.00157011873458
Epoch: 850 / 2000 Avg Loss: 0.00149563281812
Epoch: 875 / 2000 Avg Loss: 0.00142682654342
Epoch: 900 / 2000 Avg Loss: 0.00136311992526
Epoch: 925 / 2000 Avg Loss: 0.00130400997072
Epoch: 950 / 2000 Avg Loss: 0.00124901849488
Epoch: 975 / 2000 Avg Loss: 0.00119781265872
Epoch: 1000 / 2000 Avg Loss: 0.0011499459953
Epoch: 1025 / 2000 Avg Loss: 0.00110525602344
Epoch: 1050 / 2000 Avg Loss: 0.00106337419471
Epoch: 1075 / 2000 Avg Loss: 0.0010240702067
Epoch: 1100 / 2000 Avg Loss: 0.000987150153849
Epoch: 1125 / 2000 Avg Loss: 0.000952385703352
Epoch: 1150 / 2000 Avg Loss: 0.000919632454376
Epoch: 1175 / 2000 Avg Loss: 0.000888720250449
Epoch: 1200 / 2000 Avg Loss: 0.000859495555058
Epoch: 1225 / 2000 Avg Loss: 0.000831845769325
Epoch: 1250 / 2000 Avg Loss: 0.000805655775201
Epoch: 1275 / 2000 Avg Loss: 0.000780806256307
Epoch: 1300 / 2000 Avg Loss: 0.000757210611686
Epoch: 1325 / 2000 Avg Loss: 0.00073479043605
Epoch: 1350 / 2000 Avg Loss: 0.000713438234874
Epoch: 1375 / 2000 Avg Loss: 0.000693112082136
Epoch: 1400 / 2000 Avg Loss: 0.000673737627738
Epoch: 1425 / 2000 Avg Loss: 0.000655235576276
Epoch: 1450 / 2000 Avg Loss: 0.000637567835559
Epoch: 1475 / 2000 Avg Loss: 0.000620676918261
Epoch: 1500 / 2000 Avg Loss: 0.000604522425069
Epoch: 1525 / 2000 Avg Loss: 0.000589041773236
Epoch: 1550 / 2000 Avg Loss: 0.000574222170835
